from pyspark.sql import SparkSession                                          # I import SparkSession to initialize and manage my Spark application

# 1. Initialize SparkSession for my BBQ2GO null‑drop demo
spark = SparkSession.builder \
    .appName("BBQ2GO_DropNulls_Demo") \
    .getOrCreate()

# 2. Create a sample DataFrame of BBQ2GO orders with some null fields
data = [
    (1001, "Brisket Sandwich",    2, 12.50),
    (1002, None,                   1,  8.75),   # missing item
    (1003, "Loaded Brisket Fries", None, 9.25),  # missing quantity
    (1004, None,                   None, None)   # all nulls
]
columns = ["order_id", "item", "quantity", "price"]
df = spark.createDataFrame(data, columns)                                     # I build DataFrame with explicit nulls
print("Original DataFrame with nulls:")
df.show()

# 3. Drop any row containing at least one null (the default behavior)
df_drop_any = df.na.drop()                                                    # I remove rows where any column is null
print("After df.na.drop() [drop any null]:")
df_drop_any.show()

# 4. Drop only rows where all columns are null
df_drop_all = df.na.drop("all")                                               # I remove rows only if every column is null
print("After df.na.drop('all') [drop only if all null]:")
df_drop_all.show()

# 5. Drop rows where specific critical columns contain nulls
df_drop_subset = df.na.drop("any", subset=["item", "quantity"])               # I drop rows missing either item or quantity
print("After df.na.drop('any', subset=['item','quantity']):")
df_drop_subset.show()

# 6. Stop Spark session now that null‑drop demo is complete
spark.stop()                                                                   # I shut down Spark to free resources
